\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsfonts}
\usepackage{amsmath} 
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{indentfirst} 
\usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage{float}

\usepackage{geometry} % Меняем поля страницы
\geometry{left=2cm}% левое поле
\geometry{right=1.5cm}% правое поле
\geometry{top=1.5cm}% верхнее поле
\geometry{bottom=3cm}% нижнее поле

\graphicspath{{./img/}}

\newtheoremstyle{mythm}  % follow `plain` defaults but change HEADSPACE.
  	{\topsep}% measure of space to leave above the theorem. E.g.: 3pt
	{\topsep}% measure of space to leave below the theorem. E.g.: 3pt
	{\itshape}% name of font to use in the body of the theorem
	{0pt}% measure of space to indent
	{\bfseries}% name of head font
	{}% punctuation between head and body
	{\newline}% space after theorem head; " " = normal interword space
	{\thmname{#1}\thmnumber{ #2.}\thmnote{ #3}}% Manually specify head

\theoremstyle{mythm}
\newtheorem{theorem}{Теорема}

\begin{document}
	% Титульный лист
	\begin{titlepage}
		\begin{center}
			Санкт-Петербургский политехнический университет Петра Великого \\ Физико-механический институт \\ Высшая школа прикладной математики и вычислительной физики
		\end{center}
		\vspace{10em}
		\begin{center}
			\Large Курсовая работа \\ по дисциплине "Численные методы"
		\end{center}
		\vspace{1em}
		\begin{center}
			\Huge Сравнение решения задачи Коши для ОДУ 1 порядка модифицированным методом Эйлера и явным методом Адамса 2 порядка.
		\end{center}
		\vspace{15em}
		{\Large 
			
			Выполнил: \hfill студент гр. 5030102/00003 Красников Р.А.
			\vspace{1em}
			
			Преподаватель: \hfill Добрецова С.Б.}
		\vspace{\fill}
		\begin{center}
			Санкт-Петербург \\ 2022
		\end{center}
	\end{titlepage}
	\newpage
	
	\tableofcontents
	
	\newpage
	
	\section{Задание.}
	
	Дано обыкновенное дифференциальное уравнение 1 порядка на отрезке. Поставить задачу Коши и найти ее решение
	\begin{enumerate}
		\item модифицированным методом Эйлера
		\item явным методом Адамса 2 порядка
	\end{enumerate}
	
	Провести сравнение результатов по графикам ошибок на отрезке для двух значений шага. Исследовать зависимость нормы погрешности от возмущения начальных условий.
	
	\section{Постановка задачи.}
	
	Дано обыкновенное дифференциальное уравнение 1 порядка
	\begin{equation}
		F(x,y,y')=0,\ x\in[a,b]
	\end{equation}
	разрешимое относительно старшей производной, то есть оно может быть записано в виде
	\begin{equation} \label{eq}
		y'=f(x,y),\ x\in[a,b]
	\end{equation}
	Также задано начальное условие
	\begin{equation} \label{cond}
		y(a)=y_0
	\end{equation}
	Известно, что существует единственная функция $\varphi(x):[a,b]\rightarrow\mathbb{R}$ -- решение задачи Коши \eqref{eq}, \eqref{cond}, т.е.
	\begin{equation}
		\begin{gathered}
			\forall x\in[a,b]: \varphi'(x)=f(x,\varphi(x)),\\
			\varphi(a)=y_0
		\end{gathered}
	\end{equation}

	Для численного решения задачи также задана сетка $\{x_i\}_{i=0}^n$.
	
	Необходимо найти сеточную функцию $\{x_i,y_i\}_{i=0}^n$ такую, что
	\begin{equation}
		\forall i=\overline{0,n}: y_i\approx\varphi(x_i)
	\end{equation}

	\section{Предварительный анализ задачи.}
	
	Пусть поставлена задача Коши 
	\begin{equation} \label{eq:cauchy_task}
		\begin{cases}
			y'=f(x,y),\ x\in[a,b],\\
			y(a)=y_0
		\end{cases}
	\end{equation}
	
	\subsection{Переход к модифицированному методу Эйлера.}
	
	Зададимся целью построить явный одношаговый метод 2 порядка:
	\begin{equation} \label{scheme}
		y_{k+1}=\Phi_f(h,x_{k},y_{k})
	\end{equation}
	В предположении достаточной гладкости функции $y(x)$ запишем ее разложение по формуле Тейлора в окрестности точки $x$, удерживая слагаемые до $o(h^2)$, поскольку строится метод 2 порядка
	\begin{equation} \label{taylor}
		y(x+h)=y(x)+\underbrace{\dfrac{h}{1!}y'(x)+\dfrac{h^2}{2!}y''(x)}_{\Delta y(x)}+o(h^2)
	\end{equation} 

	Идея методов Рунге-Кутты, к которым относится и модифицированный метод Эйлера, состоит в отыскании функции $\delta y(x)$, отличающейся от $\Delta y(x)$ на бесконечно малую нужного порядка малости, т.е.
	\begin{equation}
		\delta y(x) = \Delta y(x) + o(h^2)
	\end{equation}
	Если удастся найти $\delta y(x)$ в виде
	\begin{equation}
		\delta y(x) = h\sum\limits_{i=1}^l \rho_if(x+\delta x_i, y + \delta y_i)
	\end{equation}
	то отбрасыванием $o(h^2)$ из \eqref{taylor} и заменой $y(x+h) \rightarrow y_{k+1}$, $y(x)\rightarrow y_k$, $\delta(x)\rightarrow \delta(x_k)$ получим метод 2 порядка, удовлетворяющий виду \eqref{scheme}
	\begin{equation}
		y_{k+1} = y_{k} + \delta y(x_{k})
	\end{equation}
	
	Будем искать $\delta y(x_k)$ в виде
	\begin{equation}
		\begin{gathered}
			\delta y(x_k) = h(\rho_1K_1 + \rho_2K_2)\\
			\begin{dcases}
				K_1 = f(x_k,y_k),\\
				K_2 = f(x_k + \alpha_2h, y_k + \beta_{21}K_1)
			\end{dcases}
		\end{gathered}		
	\end{equation}

	Формулы для неизвестных получаются следующим образом. Сравнивая выражения для $\Delta y(x)$ и $\delta y(x)$
	\begin{gather}
		\Delta y(x) = hf + \frac{h^2}{2}\bigg(\frac{\partial f}{\partial x} + f\frac{\partial f}{\partial y}\bigg)\\
		\delta y(x) = h(\rho_1 + \rho_2)f + h^2\alpha_2\rho_2\frac{\partial f}{\partial x}+h^2\beta_{21}\rho_2f\frac{\partial f}{\partial y}+o(h^2)
	\end{gather} 
	получаем СЛАУ
	\begin{equation}
		\begin{dcases}
			\rho_1 + \rho_2 = 1,\\
			\alpha_2\rho_2 = \frac{1}{2},\\
			\beta_{21}\rho_2 = \frac{1}{2}
		\end{dcases}
	\end{equation}
	В этой СЛАУ число неизвестных больше числа уравнений, поэтому полагаем $\rho_2=p$, тогда $\rho_1=1-p$, $\alpha_2=\beta_{21}=\dfrac{1}{2p}$.

	Модифицированным методом Эйлера называется метод Рунге-Кутты 2 порядка с $p=1$. Подставляя $p=1$ получаем следующие значения для неизвестных:
	\begin{equation}
		\rho_1=0,\ \rho_2=1,\ \alpha_2=\beta_{21}=\dfrac{1}{2}
	\end{equation}

	\subsection{Переход к явному методу Адамса 2 порядка.}
	
	Проинтегрируем уравнение $y'=f(x,y)$ по отрезку $[x_{k-1}, x_k]$:
	\begin{equation} \label{int_adams}
		y_k=y_{k-1}+\int\limits_{x_{k-1}}^{x_k}f(x,y(x))dx=y_{k-1}+\int\limits_{x_{k-1}}^{x_k}F(x)dx
	\end{equation}
	Будем строить явный метод 2 порядка.
	
	Аппроксимируем функцию $F(x)$ интерполяционным полиномом Лагранжа $L_1(x)$ на сеточной функции $\{\overline{x}_i, F_i\}_{i=0}^1$, где $F_i= F(\overline{x}_i)$.
	\begin{equation}
		F(x) = L_1(x) + R_1(x)
	\end{equation} 
	и подставим в \eqref{int_adams}
	\begin{equation}
		y_k=y_{k-1}+\underbrace{\int\limits_{x_{k-1}}^{x_k}L_1(x)dx}_{I_1} + \underbrace{\int\limits_{x_{k-1}}^{x_k}R_1(x)dx}_{I_2}
	\end{equation}
	Здесь $\{\overline{x}_i\}_{i=0}^2$ -- сетка, такая что $\overline{x}_2=\overline{x}_0+2h=x_0+hk=x_k$, (то есть эта сетка состоит из точек $x_{k-2}$, $x_{k-1}$, $x_{k}$). При этом интерполяционный полином строится по точкам $x_{k-1}$ и $x_{k-2}$ (а не по $x_{k}$ и $x_{k-1}$), поскольку строим явный метод, то есть значение функции $F(x)$ в узле $x_k$ экстраполируется. Не для всех $x_k$ такую сетку можно построить, поэтому для явного метода Адамса 2 порядка требуется вычислять одну разгонную точку.
	Пренебрежем интегралом $I_2$ от остаточного члена полинома Лагранжа, а в интеграле $I_1$ выполним замену переменной по формуле $x=\overline{x}_0+ht$
	\begin{equation}
		I_1=\int\limits_{\overline{x}_{1}}^{\overline{x}_2}L_1(x)dx=h\int\limits_{1}^{2}L_m(\overline{x}_0+ht)dt
	\end{equation}
	По формуле для полинома Лагранжа на равномерной сетке
	\begin{equation}
		I_1=h\int\limits_{1}^{2}\sum\limits_{j=0}^1F_j\frac{(-1)^{1-j}}{(1-j)!j!}\frac{\overline{\omega}(t)}{t-j}dt,
	\end{equation}
	где $\displaystyle \overline{\omega}(t)=t(t-1)$. Упростим выражение
	\begin{equation}
		\begin{gathered}
		I_1=h\sum\limits_{j=0}^1\underbrace{\bigg[\frac{(-1)^{1-j}}{(1-j)!j!}\int\limits_{1}^2\frac{\overline{\omega}(t)}{t-j}dt\bigg]}_{\beta_j}F_j=\\=h\bigg(\underbrace{\frac{-1}{1!\cdot0!}\int\limits_1^2(t-1)dt}_{\beta_0}\cdot F_0 + \underbrace{\frac{1}{0!\cdot1!}\int\limits_1^2tdt}_{\beta_1}\cdot F_1\bigg)=h(\beta_{0}F_0+\beta_{1}F_1)
		\end{gathered}
	\end{equation}
	Получили метод
	\begin{equation}
		y_{k}=y_{k-1}+h(\beta_0f(x_{k-2}, y_{k-2})+\beta_1f(x_{k-1}, y_{k-1}))
	\end{equation} 
	
	Вычисляя значения $\beta_0$ и $\beta_1$, получаем
	\begin{equation}
		\beta_0 = -\dfrac{1}{2},\ \beta_1 = \dfrac{3}{2}
	\end{equation}

	\section{Алгоритмы методов.}
	
	\subsection{Условия применимости исследуемых методов.}
	
	Из предварительного анализа задачи видно, что если задача Коши \eqref{eq:cauchy_task} имеет единственное решение и построенные по исследуемым методам последовательности сходятся, то их предел есть точное решение задачи Коши. Таким образом условие применимости обоих методов
	\begin{enumerate}
		\item Задача Коши \eqref{eq:cauchy_task} имеет единственное решение
	\end{enumerate}

	В таком виде условие применимости может быть не удобным в применении к конкретной задаче, поэтому можно воспользоваться теоремами
	\begin{theorem}[О единственности решения задачи Коши.]
		Если функция $f(x,y)$ удовлетворяет условию Липшица по y в прямоугольнике $\Pi = [a, b] \times [c, d]$, т.е.
		\begin{equation}
			\exists L > 0\ \forall x \in [a,b]\ \forall y_1,y_2\in[c,d]\ |f(x,y_1) - f(x,y_2)| < L|y_1-y_2|,
		\end{equation} 
		то решение задачи Коши \eqref{eq:cauchy_task} единственно.
	\end{theorem}
	\begin{theorem}[О достаточном условии липшицевости функции $f(x,y)$.]
		Если функция $f(x,y)$ имеет непрерывную частную производную $\dfrac{\partial f}{\partial y}$ в прямоугольнике $\Pi = [a, b] \times [c, d]$, то $f(x,y)$ удовлетворяет условию Липшица по $y$ в прямоугольнике $\Pi$, и для константы Липшица справедливо выражение
		\begin{equation}
			L = \max\limits_{(x,y)\in \Pi}\dfrac{\partial f}{\partial y}\bigg|_{(x,y)}
		\end{equation}
	\end{theorem}
	
	При анализе задачи область значений решения неизвестна. В связи с этим нужно потребовать, чтобы условия теорем выполнялись для достаточно больших отрезков $[c,d] \supset E(\varphi)$, где $E(\varphi)$ -- область значений точного решения $\varphi(x)$ задачи Коши \eqref{eq:cauchy_task}.
	
	При этом ограниченности частной производной по $x$ требовать не обязательно. Например, единственным решением задачи Коши
	\begin{equation}
		\begin{dcases}
			y' = \sqrt{x}, x\in[0,1],\\
			y(0) = 0
		\end{dcases}
	\end{equation}
	является функция $\varphi(x)=\dfrac{2}{3}x\sqrt{x}$, но $\dfrac{\partial}{\partial x} (\sqrt{x})=\dfrac{1}{2\sqrt{x}} \xrightarrow[x\rightarrow+0]{}+\infty$, а в самой точке $0$ функция $\dfrac{1}{2\sqrt{x}}$ и вовсе не определена. 
	
	Таким образом, достаточное условие единственности задачи Коши
	\begin{enumerate}
		\item $\dfrac{\partial f}{\partial y}$ непрерывна
	\end{enumerate}
	
	\subsection{Этапы решения поставленной задачи Коши исследуемыми методами.}
	
	В соответствии с постановкой задачи, дана задача Коши для дифференциального уравнения, не разрешенного, но разрешимого, относительно производной. Поэтому первый шаг состоит в разрешении дифференциального уравнения относительно производной.
	
	После разрешения уравнения относительно производной, получаем задачу Коши в виде \eqref{eq:cauchy_task} и к ней уже непосредственно применяем исследуемые методы. 
	
	Таким образом, этапы решения задачи следующие:
	\begin{enumerate}
		\item Разрешить дифференциальное уравнение относительно производной
		\item Применить численный метод решения задачи Коши для дифференциального уравнения 1 порядка, разрешенного относительно производной
	\end{enumerate}
	
	\subsection{Алгоритмы исследуемых методов.}
	
	Для обоих исследуемых алгоритмов первый шаг одинаков:
	\begin{enumerate}
		\item Ввести функцию $f(x,y)$, начальное условие $y_0$, отрезок $[a,b]$, число отрезков разбиения $m$.
	\end{enumerate}
	Число точек равномерной сетки составит $m+1$, шаг $h=\dfrac{b-a}{m}$, а узлы сетки вычисляются по формуле $x_i=a+ih$.	
	
	\subsubsection{Модифицированный метод Ньютона.}
	
	\begin{enumerate}
		\setcounter{enumi}{1}
		\item Для всех $i=\overline{1,m}$ вычислить
		\begin{equation} \label{euler_method}
			y_i = y_{i-1} + hf(x_{i-1} + \frac{h}{2}, y_{i-1}+\frac{h}{2}f(x_{i-1},y_{i-1}))
		\end{equation}
		\item $\{(x_i,y_i)\}_{i=0}^m$ -- искомая сеточная функция
	\end{enumerate}

	\subsubsection{Явный метод Адамса 2 порядка.}
	
	\begin{enumerate}
		\setcounter{enumi}{1}
		\item Вычислить разгонную точку (с помощью модифицированного метода Эйлера):
		\begin{equation} \label{euler_boost}
			y_1 = y_0 + hf(x_0 + \frac{h}{2}, y_0+\frac{h}{2}f(x_0,y_0)),
		\end{equation}
		\item Для всех $i=\overline{2,m}$ вычислить
		\begin{equation} \label{adams_method}
			y_i = y_{i-1} + \frac{h}{2}(3f(x_{i-1},y_{i-1})-f(x_{i-2}, y_{i-2}))
		\end{equation}
		\item $\{(x_i,y_i)\}_{i=0}^m$ -- искомая сеточная функция
	\end{enumerate}
	
	\subsection{Теоретическая оценка результатов исследования.}
	
	\subsubsection{Исследование полученных решений и их погрешностей.}
	
	Из алгоритмов исследуемых методов видно, что при решении задачи Коши модифицированным методом Эйлера нужно на каждой итерации дважды вычислять значение функции $f(x,y)$, в то время как при решении явным методом Адамса 2 порядка -- только один раз (кроме итерации для вычисления разгонной точки и первой итерации вычисления непосредственно по формуле явного метода Адамса \eqref{adams_method}).
	
	В связи с этим ожидается, что решение полученное явным методом Адамса 2 порядка будет не точнее решения, полученного модифицированным методом Эйлера для одного значения шага, ведь в противном случае получится, что вычислений по методу Адамса нужно меньше и решение при этом точнее, что поставило бы под вопрос эффективность модифицированного метода Эйлера. 
	
	Также оба исследуемых метода имеют 2 порядок аппроксимации, поэтому ожидается, что норма глобальной погрешности при уменьшении шага в 2 раза уменьшится приблизительно в 4 раза.
	
	\subsubsection{Исследование зависимости нормы погрешности от возмущения начального условия.}
	
	\paragraph{Устойчивость задачи Коши по начальному условию.} 
	
	Рассмотрим задачу Коши, зависящую от параметра
	\begin{equation} \label{eq:cauchy_param}
		\begin{cases}
			y' = f(x,y,\mu),\\
			y(x_0,\mu) = y_0
		\end{cases}
	\end{equation}
	Из курса математического анализа известна следующая теорема
	\begin{theorem}[О непрерывности решения задачи Коши по параметру.] \label{th:param}
		Если функция $f(x,y,\mu)$ и ее частная производная $\dfrac{\partial f}{\partial y}$ непрерывны в некоторой области $G$ и точка $(x_0, y_0, \mu_0)\in G$, то решение задачи Коши \eqref{eq:cauchy_param} непрерывно в некоторой области $D =\{(x,\mu)\in\mathbb{R}^2\ |\ |x-x_0|<\delta_1, |\mu-\mu_0|<\delta_2\}$
	\end{theorem}

	Теперь в задаче Коши
	\begin{equation}
		\begin{cases} \label{eq:cauchy_task_x0}
			y' = f(x,y),\\
			y(a) = y_0
		\end{cases}
	\end{equation} считаем $y_0$ параметром. Выполним замену переменных по формулам $\widetilde{x} = x - a$, $\widetilde{y}(\widetilde{x}, y_0) = y(x) - y_0 = y(\widetilde{x} + a) - y_0$. Тогда правая часть запишется в виде $f(\widetilde{x} + a, \widetilde{y} + y_0)$.

	В новых переменных имеем задачу Коши, зависящую от параметра $y_0$
	\begin{equation}
		\begin{cases} \label{eq:cauchy_task_tilde}
			y' = f(\widetilde{x} + a, \widetilde{y} + y_0),\\
			\widetilde{y}(0, y_0) = 0
		\end{cases}
	\end{equation}

	Наконец, чтобы в точности удовлетворить условию теоремы \ref{th:param}, можно положить $\widetilde{f}(\widetilde{x}, \widetilde{y}, y_0) = f(\widetilde{x} + a, \widetilde{y} + y_0)$. Из этой теоремы следует, что в условиях достаточной гладкости функции $\widetilde{f}(\widetilde{x}, \widetilde{y}, y_0)$ решение задачи Коши \eqref{eq:cauchy_task_tilde} непрерывно по параметру $y_0$ -- то есть начальному условию задачи Коши \eqref{eq:cauchy_task}.
	
	Это значит, что интегральная кривая, проходящая через точку $(a, y_0 \pm \Delta y_0)$ для небольших $\Delta y_0$ расположена на небольшом расстоянии от кривой, проходящей через точку $(a, y_0)$. Если в начальное условие задачи внесено небольшое возмущение, то интегральная кривая, являющаяся пределом последовательности, построенной каждым из методов, при $h \rightarrow 0$, не сильно отстоит от точного решения.
	
	\paragraph{Устойчивость модифицированного метода Эйлера по начальному условию.}
	
	Рассмотрим для произвольного числа $n+1$ точек равномерной сетки численное решение $\{x_i, y_i\}_{i=0}^n$, полученное модифицированным методом Эйлера для задачи Коши с невозмущенным начальным условием $y(a)=y_0$ и численное решение $\{x_i, \widetilde{y}_i\}_{i=0}^n$, полученное для возмущенного начального условия $y(a)=\widetilde{y}_0=y_0 + \Delta y_0$. Обозначим
	\begin{equation}
		\begin{gathered}
			F(x,y) = f(x + \frac{h}{2}, y+\frac{h}{2}f(x,y)),\\
			F_k = F(x_k, y_k),\\
			\widetilde{F}_k = F(x_k, \widetilde{y}_k)
		\end{gathered}
	\end{equation}
	Теперь, в точке $x_1$ получим значения $y_1$, $\widetilde{y}_1$ решений невозмущенной и возмущенной задачи соответственно по формулам
	\begin{equation} \label{y1_dist_notdist}
		\begin{gathered}
			y_1 = y_0 + hF_0,\\
			\widetilde{y}_1 = \widetilde{y}_0 + h\widetilde{F}_0
		\end{gathered}
	\end{equation}
	Обозначим $\varepsilon_i = |y_i - \widetilde{y}_i|$ -- ошибка в $i$-ом узле сетки, обусловленная внесением возмущения в начальное условие. Тогда из \eqref{y1_dist_notdist} вычитанием и применением неравенства треугольника получаем
	\begin{equation} \label{epsilons10}
		\varepsilon_1 \leq \varepsilon_0  + h|F_0-\widetilde{F_0}|
	\end{equation}
	Оценим величину $|F_i-\widetilde{F_i}|$ для произвольного $i$, пользуясь условием Липшица по $y$ для $f(x,y)$:
	\begin{equation}
		\begin{gathered}
			|F_i-\widetilde{F_i}|=\\=|f(x_i + \frac{h}{2}, y_i+\frac{h}{2}f(x_i,y_i))-f(x_i + \frac{h}{2}, \widetilde{y}_i+\frac{h}{2}f(x_i,\widetilde{y}_i))| \leq \\ \leq L|y_i-\widetilde{y}_i|+L\frac{h}{2}|f(x_i,y_i)-f(x_i,\widetilde{y}_i)| \leq \\ \leq
			L\varepsilon_i + L^2\frac{h}{2}|y_i-\widetilde{y}_i| = \\ =
			L\varepsilon_i + L^2\frac{h}{2}\varepsilon_i = \\ =
			\bigg(L + \frac{h}{2}L^2\bigg)\varepsilon_i
		\end{gathered}
	\end{equation}
	Таким образом, имеем
	\begin{equation}
		|F_i-\widetilde{F}_i| \leq \bigg(L + \frac{h}{2}L^2\bigg)\varepsilon_i
	\end{equation}
	Подставим это выражение в \eqref{epsilons10}, имеем
	\begin{equation} \label{eps1limit}
		\varepsilon_1 \leq \varepsilon_0 + h\bigg(L+\frac{h}{2}L^2\bigg)\varepsilon_0=\bigg(1 + hL + \frac{h^2}{2}L^2\bigg)\varepsilon_0
	\end{equation}
	Для произвольных $i$ \eqref{eps1limit} записывается аналогично, поэтому
	\begin{equation}
		\begin{gathered}
			\varepsilon_n \leq \bigg(1 + hL + \frac{h^2}{2}L^2\bigg)\varepsilon_{n-1} \leq \\ \leq \bigg(1 + hL + \frac{h^2}{2}L^2\bigg)^2\varepsilon_{n-2} \leq \\ \leq \dots \leq \\ \leq  \bigg(1 + hL + \frac{h^2}{2}L^2\bigg)^n\varepsilon_{0} \leq \\ \leq e^{nhL}\varepsilon_{0} = e^{(b-a)L}\varepsilon_{0}=e^{(b-a)L}|\Delta y_0|
		\end{gathered}
	\end{equation}

	Таким образом, в численном решении возмущенной задачи Коши накапливается "дополнительная"\ (вызванная возмущением начального условия) погрешность, ограниченная сверху линейной зависимостью от абсолютной величины возмущения начального условия $\Delta y_0$. Ожидается, что модифицированный метод Эйлера устойчив по начальному условию
	
	\paragraph{Устойчивость явного метода Адамса 2 порядка по начальному условию.}
	
	Рассмотрение происходит во введенных ранее обозначениях. 
	
	Теперь, в точке $x_1$ значения $y_1$, $\widetilde{y}_1$ решений невозмущенной и возмущенной задачи соответственно получаются по формулам
	\begin{equation} \label{adamsdist}
		\begin{gathered}
			y_i = y_{i-1} + \frac{h}{2}(3f(x_{i-1}, y_{i-1})-f(x_{i-2}, y_{i-2})),\\
			\widetilde{y}_i = y_i = \widetilde{y}_{i-1} + \frac{h}{2}(3f(x_{i-1}, \widetilde{y}_{i-1})-f(x_{i-2}, \widetilde{y}_{i-2}))
		\end{gathered}
	\end{equation}
	Далее из \eqref{adamsdist} для $i=n$ вычитанием, применением неравенства треугольника и условия Липшица по $y$ для функции $f(x,y)$ получаем
	\begin{equation}
		\begin{gathered}
			\varepsilon_i \leq \varepsilon_{i-1} + \frac{3hL}{2}\varepsilon_{i-1} + \frac{hL}{2}\varepsilon_{i-2} \leq \\ \leq
			\varepsilon_{i-1} + \frac{3hL}{2}\varepsilon_{i-1} + \frac{hL}{2}\varepsilon_{i-1} \leq \\ \leq
			(1 + 2hL)\varepsilon_{i-1} \leq \\ \leq (1+2hL)^2\varepsilon_{i-2} \leq \\ \leq \dots \leq \\ \leq
			(1 + 2hL)^{n-1}\varepsilon_{1} \leq \\ \leq e^{2(n-1)hL}\varepsilon_{1} \leq \\ \leq
			e^{2(n-1)hL}e^{hL}\varepsilon_0 \leq \\ \leq e^{2(n-1)hL}e^{2hL}\varepsilon_0 = e^{2nhL}\varepsilon_0 = 
			e^{2(b-a)L}\varepsilon_0 = e^{2(b-a)L}|\Delta y_0|
		\end{gathered}		
	\end{equation}
	
	Таким образом, в отношении явного метода Адамса 2 порядка можно сделать аналогичный вывод.
	
	Ожидается, что оба исследуемых метода устойчивы по начальному условию.
	
	\section{Контрольные тесты.}
	
	\subsection{Решаемая задача Коши.}
	
	Исследование проведено для задачи Коши
	\begin{equation} \label{mytask}
		\begin{cases}
			xy'-2x^2\sqrt{y}=4y,\ x\in[1,2],\\
			y(1)=1
		\end{cases}
	\end{equation}
	Ее точное решение $\varphi(x)=x^4(\ln x+1)^2$.
	
	\subsection{Численные эксперименты.}
	
	Графики полученных решений и их ошибок построены для шагов $h_1=0.2$ и $h_2=0.1$.
	
	Для исследования зависимости нормы погрешности от возмущения начальных условий были выбраны значения шага $h=0.1$ и возмущений начального условия $\Delta y_0=(1.1)^{-i}, i=\overline{0,85}$. Были проведены 2 эксперимента: для возмущений вида $y(a)=y_0+\Delta y_0$ и $y(a)=y_0-\Delta y_0$. 
	
	\section{Численный анализ.}
	
	\subsection{Анализ полученных решений и их погрешностей.} \label{analysis:ans}
	
	Полученные в процессе исследования графики решений задачи Коши \eqref{mytask} представлены на рисунке \ref{fig:ans}.
	
	\begin{figure}[H]\centering
		\includegraphics[width=.49\textwidth]{ans_Euler}
		\includegraphics[width=.49\textwidth]{ans_Adams}
		\caption{Решения, полученные модифицированным методом Эйлера (слева) и явным методом Адамса 2 порядка (справа).}\label{fig:ans}
	\end{figure}

	На графиках полученных решений видно, что для всех шагов оба метода дают решение, значения которого в узлах меньше, чем точное. Приближенное решение "не успевает"\ за ростом точного. Графики погрешностей данных решений представлены на рисунке \ref{fig:err}.
	
	\begin{figure}[H]\centering
		\includegraphics[width=.75\textwidth]{err_on_x}
		\caption{Ошибки решений, полученных модифицированным методом Эйлера и явным методом Адамса 2 порядка.}\label{fig:err}
	\end{figure}

	На графиках погрешностей полученных решений видно, что погрешности для всех шагов и методов возрастают быстрее, чем линейно. Модифицированный метод Эйлера и исследованный явный метод Адамса имеют 2 порядок аппроксимации, значит при уменьшении шага в 2 раза (c $0.2$ до $0.1$) ошибка должна была уменьшиться приблизительно в 4 раза. 
	
	Например, в точке $b=2$ видно, что для модифицированного метода Эйлера ошибка уменьшилась приблизительно в $5.4/1.7\approx3.2$ раза, а для явного метода Адамса 2 порядка -- приблизительно в $8.5/3.0\approx2.8$ раза. Это объясняется тем, что выражение для погрешности имеет вид $Ch^2$ только при достаточно малых $h$, а в общем случае имеет вид $Ch^2 + o(h^2)$ -- в проведенном эксперименте $h$ еще достаточно велико и $o(h^2)$ дает ощутимую поправку. На рисунке \ref{fig:err_on_h} можно видеть, что для шага $0.1$ и больше зависимость действительно отклоняется от прямой $y=Ch^2$ вогнутым образом.
	
	\begin{figure}[H]\centering
		\includegraphics[width=.75\textwidth]{err_on_h}
		\caption{Зависимость бесконечной нормы погрешности решения от шага.}\label{fig:err_on_h}
	\end{figure}
	
	Также отметим, что для одних и тех же значений шага модифицированный метод Эйлера дал более точное решение, чем явный метод Адамса 2 порядка. Этот результат вполне ожидаем, поскольку модифицированный метод Эйлера требует двух вычислений значения функции $f(x,y)$ на каждой итерации, в то время как явный метод Адамса 2 порядка -- только одного вычисления (кроме итерации для вычисления разгонной точки и первой итерации вычисления непосредственно по формуле явного метода Адамса \eqref{adams_method}).
	
	\subsection{Анализ зависимости нормы погрешности от возмущения начальных условий.}
	
	Полученные в процессе исследования зависимости нормы погрешности от возмущения начальных условий графики приведены на рисунке \ref{fig:err_on_dy}.
	
	\begin{figure}[H]\centering
		\includegraphics[width=.49\textwidth]{err_on_dyplus}
		\includegraphics[width=.49\textwidth]{err_on_dyminus}
		\caption{Зависимости бесконечной нормы погрешности решения c шагом 0.1 от возмущения начального условия вида $y(1) = 1.0+\Delta y_0$ (слева) и $y(1) = 1.0-\Delta y_0$ (справа).}\label{fig:err_on_dy}
	\end{figure}

	На графиках исследуемой зависимости видно, что для обоих методов имеет место асимптота при $\Delta y_0 \rightarrow 0$, притом равная погрешности соответствующих методов для значения шага $0.1$, приведенной на рисунке \ref{fig:err}. Для $\Delta y_0 \leq 10^{-3}$ отклонение погрешности от асимптоты незначительное. 
	
	Интересно отметить, что для возмущения начального условия вида $y(1)=1.0+\Delta y_0$ для обоих методов наблюдается минимум в окрестности точки $\Delta y_0 = 10^{-1}$, притом для возмущения вида $y(1)=1.0-\Delta y_0$ такого минимума не наблюдается. Это можно объяснить, проанализировав полученные методами решения для двух значений шага, графики которых приведены на рисунке \ref{fig:ans}. На графиках видно, что численное решение всюду меньше, чем точное и, как уже было отмечено в пункте \ref{analysis:ans}, "не успевает"\ за ростом точного решения. Внося погрешность вида $y(1)=1.0+\Delta y_0$, мы получаем решения, чья первая точка расположена несколько выше, чем первая точка точного решения, тем самым компенсируя более быстрый рост точного решения по сравнению с численным и, при $\Delta y_0 \approx 10^{-1}$ такое возмущение наиболее компенсирует недостаточную скорость роста численного решения, и глобальная погрешность оказывается на 1 порядок меньше, чем при решении задачи Коши с невозмущенным начальным условием. Графики решений и их погрешностей для  начальных условий $y(1)=1.0$ и $y(1)=1.0+0.1$ приведены на рисунке \ref{fig:disturbed}.
	
	\begin{figure}[H]\centering
		\includegraphics[width=.49\textwidth]{ans_Euler_disturbed}
		\includegraphics[width=.49\textwidth]{ans_Adams_disturbed}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=.49\textwidth]{err_Euler_disturbed}
		\includegraphics[width=.49\textwidth]{err_Adams_disturbed}
		\caption{Решения задачи Коши с начальным условием $y(1)=1.0$ и $y(1)=1.0+0.1$ модифицированным методом Эйлера (слева) и явным методом Адамса 2 порядка (справа).}\label{fig:disturbed}
	\end{figure}

	На рисунке \ref{fig:disturbed} видно, что благодаря возмущению начального условия вида $y(1)=1.0 + 0.1$, решение "поднялось"\ вверх и это обстоятельство компенсирует недостаточную скорость роста численного решения. Так для метода Адамса минимум зависимости погрешности от возмущения начального условия расположен близко к точке $\Delta y_0 = 10^{-1}$ и численное решение задачи с возмущенным начальным условием пересекает точное, а погрешность проходит через нуль. Глобальная погрешность за счет возмущения начального условия уменьшилась в $3/0.5=6$ раз.
	
	Для модифицированного метода Эйлера возмущение $\Delta y_0 = 10^{-1}$ в соответствии с рисунком \ref{fig:err_on_dy} оказывается несколько избыточным и для такого возмущения численное решение целиком лежит выше точного, однако глобальная погрешность по модулю также оказалась меньше, чем глобальная погрешность решения невозмущенной задачи.
	
	Вышеприведенный анализ подтверждает справедливость гипотезы о причине возникновения минимума глобальной погрешности при внесении положительного возмущения в начальное условие. Однако, зависимость была исследована для шага $0.1$, при котором вид зависимости погрешности обоих исследуемых методов от шага далек от асимптотического $Ch^2$. Чтобы убедиться, что это обстоятельство не влияет на вид исследованной зависимости, были также построены графики зависимости глобальной погрешности от возмущения начального условия (положительного) для шагов $10^{-3}$ и $10^{-5}$. Графики приведены на рисунке \ref{fig:disturbed_lowh}.
	
	\begin{figure}[H]
		\includegraphics[width=.49\textwidth]{err_on_dyplus_lowh}
		\includegraphics[width=.49\textwidth]{err_on_dyplus_verylowh}
		\caption{Зависимости бесконечной нормы погрешности решения c шагом $10^{-3}$ (слева) и с шагом $10^{-5}$ (справа) от возмущения начального условия вида $y(1) = 1.0+\Delta y_0$.}\label{fig:disturbed_lowh}
	\end{figure}
	
	На графиках для меньших значений шага видим, что характер зависимости остался прежним несмотря на значительно меньший шаг, при котором глобальная погрешность имеет асимптотический вид. Уменьшение шага сопровождается лишь уменьшением погрешности решения невозмущенной задачи (асимптоты зависимости погрешности возмущенной задачи от возмущения начального условия) и сдвигом точки локального минимума в сторону меньшего возмущения. В точке минимума погрешность все так же оказывается на порядок меньше, чем погрешность решения невозмущенной задачи.
	
	\section{Выводы.}
	
	По результатам проведенного исследования можно сформулировать следующие выводы:
	\begin{itemize}
		\item Методы Рунге-Кутты и, в частности, модифицированный метод Эйлера основаны на разложении функции $y(x)$ по формуле Тейлора в точке $x+h$. При построении метода подбирается функция, отличающаяся от суммы членов формулы Тейлора на бесконечно малую порядка, заданного требуемым порядком аппроксимации метода.
		\item Методы Адамса основаны на интерполяции функции $F(x)=f(x,y(x))$ многочленом Лагранжа на равномерной сетке и его интегрировании. При построении метода требуемый порядок аппроксимации достигается за счет выбора степени интерполяционного многочлена Лагранжа, или, что равносильно, шаговости метода Адамса.
		\item Явный метод Адамса 2 порядка требует вычисления разгонной точки. Эту точку следует вычислять методом, порядок аппроксимации которого не меньше 2-го. В данной работе для этого использовался модифицированный метод Эйлера -- один из методов Рунге-Кутты 2 порядка.
		\item Модифицированный метод Эйлера требует на каждой итерации дважды вычислять значение функции $f(x,y)$, в то время как явный метод Адамса 2 порядка -- только единожды (кроме вычисления разгонной точки и первой итерации самого метода Адамса).
		\item Модифицированный метод Эйлера дает более точное решение исследуемой задачи Коши, чем явный метод Адамса 2 порядка, для одного и того же шага.
		\item Оба исследуемых метода устойчивы к возмущениям начального условия ввиду того, что решение задачи Коши для ОДУ 1 порядка, разрешенного относительно производной, непрерывно по начальному условию.
		\item При определенных условиях возмущение начального условия может уменьшить глобальную погрешность решения, полученного обоими исследуемыми методами.
	\end{itemize}
	
\end{document}